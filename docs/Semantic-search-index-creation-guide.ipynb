{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search index creation \n",
    "\n",
    "### *Guide*\n",
    "\n",
    "Anton Antonov    \n",
    "September 2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to create an LLM-computed vector database over the paragraphs of relatively large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Retrieval Augmented Generation (RAG) workflow we consider:\n",
    "\n",
    "- The document collection is ingested.\n",
    "- The documents are split into chunks of relevant sizes.\n",
    "- Large Language Model (LLM) embedding vectors are obtained for all chunks.\n",
    "- A vector database is created with these embedding vectors and stored locally. Multiple local databases can be created.\n",
    "- A relevant local database is imported for use.\n",
    "- An input query is provided to a retrieval system.\n",
    "- The retrieval system retrieves relevant documents based on the query.\n",
    "- The top K documents are selected for further processing.\n",
    "- The model is fine-tuned using the selected documents.\n",
    "- The fine-tuned model generates an answer based on the query.\n",
    "- The output answer is presented to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Mermaid-JS component diagram that shows the components of performing the Retrieval Augmented Generation (RAG) workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph LocalVDB[Local Folder]\n",
    "        A(Vector Database 1)\n",
    "        B(Vector Database 2)\n",
    "        C(Vector Database N)\n",
    "    end\n",
    "    D[Ingest Vector Database] -.- LocalVDB\n",
    "    D --> E\n",
    "    E[/User Query/] --> F[Retrieval]\n",
    "    F --> G[Document Selection]\n",
    "    G -->|Top K documents| H(Model Fine-tuning)\n",
    "    H --> I[[Generation]]\n",
    "    I <-.-> LLM{{LLM}}\n",
    "    I -->J[/Output Answer/]\n",
    "    G -->|Top K passages| K(Model Fine-tuning)\n",
    "    K --> I\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this diagram:\n",
    "\n",
    "- There are multiple local vector databases that are stored and maintained locally.\n",
    "- A vector database from the local collection is selected and ingested.\n",
    "- An input query provided by the user initiates the RAG workflow.\n",
    "- The workflow then proceeds with: \n",
    "  - retrieval\n",
    "  - document selection\n",
    "  - model fine-tuning\n",
    "  - answer generation\n",
    "  - presenting the final output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use Data::Importers;\n",
    "use LLM::Functions;\n",
    "use XDG::BaseDirectory :terms;\n",
    "\n",
    "use LLM::RetrievalAugmentedGeneration;\n",
    "use LLM::RetrievalAugmentedGeneration::VectorDatabase;\n",
    "\n",
    "use Data::Reshapers;\n",
    "use Data::Summarizers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Ingest text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the transcript of the (3.5 hours) discussion [CWv1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chars => 185217 words => 28485 lines => 4580)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my $url = 'https://podscripts.co/podcasts/modern-wisdom/833-eric-weinstein-are-we-on-the-brink-of-a-revolution';\n",
    "my $url = 'https://podscripts.co/podcasts/modern-wisdom/747-eric-weinstein-why-does-the-modern-world-make-no-sense';\n",
    "my $txtEN = data-import($url, 'plaintext');\n",
    "\n",
    "text-stats($txtEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the \"proper transcript\" part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chars => 182123 words => 28138 lines => 4522)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $txtEN2 = $txtEN.substr($txtEN.index('Starting point is 00:00:00'));\n",
    "text-stats($txtEN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into paragraphs and make the paragraphs compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my @paragraphs = $txtEN2.split(/ 'Starting point is' \\h+ [\\d ** 2]+ % ':' /):g;\n",
    "@paragraphs .= map({ $_.subst(/\\n+/, \"\\n\"):g});\n",
    "@paragraphs.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li> \n",
       "                                         like that since primary school since i was 11 or 10. Yeah.\n",
       "Is there religious music that moves you?\n",
       "See, you have the major scale as the centerpiece of Western music.\n",
       "Do you need a guitar?\n",
       "We&#39;re okay.\n",
       "Wait, do you have a guitar?\n",
       "We have a guitar.\n",
       "Can we get the guitar?\n",
       "                                            </li><li> \n",
       "                                         talking about Jeffrey Epstein, everybody talking about him,\n",
       "and spend half of that time saying,\n",
       "what do we think about whoever was behind Jeffrey Epstein?\n",
       "Whatever was behind Jeffrey Epstein\n",
       "is what I think cared about gravity,\n",
       "cared about space-time, cared about physics.\n",
       "And you get to use this supposed financier as a wedge\n",
       "to be able to start to break this open?\n",
       "                                            </li><li> \n",
       "                                         is that people tend to ask clustered questions.\n",
       "And I&#39;m always looking for that interviewer\n",
       "who&#39;s going to ask me things that are just like\n",
       "people haven&#39;t heard um mostly what we do is we just do retreads of the same old\n",
       "questions and it&#39;s not a critique of either one of us as interviewers it&#39;s just we don&#39;t know how\n",
       "to get out of our traffic circle\n",
       "where we go around and around.\n",
       "I&#39;m trying to build the most exciting thing in the world,\n",
       "                                            </li><li> \n",
       "                                         Well, I saw this frustration that lawmakers had because they were getting compartmentalized. If\n",
       "you don&#39;t ask precisely the right person, precisely the right question in precisely the right way,\n",
       "you&#39;re not allowed to get an answer. You don&#39;t get an answer. But you couldn&#39;t even,\n",
       "look, if you don&#39;t know what a Ramanian manifold is,\n",
       "if you don&#39;t know what a determinant line bundle is, there&#39;s no way you can ask intelligent\n",
       "questions about alien visitation. How did they get here? There are no scientists.\n",
       "There are no relevant scientists in this story.\n",
       "Does anybody find that at all odd?\n",
       "                                            </li></ul>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% html\n",
    "@paragraphs.pick(4) ==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Make vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an empty vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(:id(\"5cb40fbb-9f69-48ca-9fc1-03ec8059ed99\"), :name(\"No747\"), :elements(0), :sources(0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $vdbObj = LLM::RetrievalAugmentedGeneration::VectorDatabase.new(name => 'No747');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an LLM access specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my $conf = llm-configuration(\"ChatGPT\", model => 'text-embedding-002');\n",
    "my $conf = llm-configuration(\"Gemini\");\n",
    "\n",
    "$conf.Hash.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the semantic index for the vector database object (an profile it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to make the semantic search index: 142.989442032 seconds.\n"
     ]
    }
   ],
   "source": [
    "my $tstart = now;\n",
    "$vdbObj.create-semantic-search-index(@paragraphs, method => 'by-max-tokens', max-tokens => 2048, e => $conf):embed;\n",
    "my $tend = now;\n",
    "say \"Time to make the semantic search index: {$tend - $tstart} seconds.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the vector database object is exported in a sub-directory of [`$XDG_DATA_HOME`](https://specifications.freedesktop.org/basedir-spec/latest/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sub-directory\n",
    "my $dirname = data-home.Str ~ '/raku/LLM/SemanticSearchIndex';\n",
    "\n",
    "# The exported vector database base file name\n",
    "my $basename = \"SemSe-{$vdbObj.id}.json\";\n",
    "\n",
    "# Corresponding IO:Path object\n",
    "my $file = IO::Path.new(:$dirname, :$basename);\n",
    "\n",
    "# Check for existence\n",
    "$file.f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export path is saved in the vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$file.Str eq $vdbObj.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li><table border=\"1\"><tr><th>061.0</th><td>Those are just words. There aren&#39;t any other routes. Okay, all right. Fair enough. There are words. There are no other routes. There are just words. That is the world&#39;s</td></tr></table></li><li><table border=\"1\"><tr><th>076.0</th><td>super smart tyrannical string theorist leader? It&#39;s not tyrannical look, I don&#39;t know how to say this because I&#39;m obviously a critic i i revere this person this is very painful for me to say you know if you if you ask me of all the people&#39;s minds on planet earth that i i revere the wonder that is ed witton&#39;s brain is beyond almost anything I can communicate, at least when you have a Beethoven or a, I don&#39;t know, an Art Tatum or a Picasso or Modigliano, you can see what it</td></tr></table></li><li><table border=\"1\"><tr><th>079.0</th><td>and whether we can leave Earth. Is there any way we can get access to more energy? Is there any way that we can reveal space-time to not be fundamental so that maybe we can do something that would be confused with going faster than light. Maybe we can reach the stars through methods that we can&#39;t understand using what we have. Why is Ed Witten guarding the exit? Ed, there are other theories. There have been theories for 40 years. I met you in your office in 1984, 85 in Princeton on a snowy day. And you threw me out of your office for what reason? Because I started talking to you about the fact that I didn&#39;t think you were right</td></tr></table></li><li><table border=\"1\"><tr><th>139.0</th><td>who says, okay, you don&#39;t want the truth. We need to have stories. Let&#39;s just make up stuff and put stuff in our pockets. How much of it is coordination? How much of it&#39;s cowardice? Well, I would rephrase that a little differently. Maybe I would say, uh, nobody smart has gotten anything to work like this in a long time. The reason we have Donald Trumpe biden is that everybody failed i failed i&#39;ve been podcasting reaching millions i&#39;ve been teaching people about all sorts of</td></tr></table></li></ul>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% html\n",
    "$vdbObj.text-chunks.pairs.pick(4).sort(*.key) ==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dimensions and data type of the obtained vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions : (283 768)\n",
      "data type  : Assoc(Vector(Atom((Str)), 283), Tuple([(Any) => 283], 283), 283)\n"
     ]
    }
   ],
   "source": [
    "say \"dimensions : \", $vdbObj.database.&dimensions;\n",
    "say \"data type  : \", deduce-type($vdbObj.database);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov, \n",
    "[\"Outlier detection in a list of numbers\"](https://rakuforprediction.wordpress.com/2022/05/29/outlier-detection-in-a-list-of-numbers/),\n",
    "(2022),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[LLM::Prompts Raku package](https://github.com/antononcube/Raku-LLM-Prompts),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[ML::FindTextualAnswer Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[Math::Nearest Raku package](https://github.com/antononcube/Raku-Math-Nearest),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[Math::DistanceFunctions Raku package](https://github.com/antononcube/Raku-Math-DistanceFunctions),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[Statistics::OutlierIdentifiers Raku package](https://github.com/antononcube/Raku-Statistics-OutlierIdentifiers),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "\n",
    "[CWv1] Chris Williamson,\n",
    "[\"Eric Weinstein - Are We On The Brink Of A Revolution? (4K)\"](https://www.youtube.com/watch?v=PYRYXhU4kxM),\n",
    "(2024),\n",
    "[YouTube/@ChrisWillx](https://www.youtube.com/@ChrisWillx).   \n",
    "([transcript](https://podscripts.co/podcasts/modern-wisdom/833-eric-weinstein-are-we-on-the-brink-of-a-revolution).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
