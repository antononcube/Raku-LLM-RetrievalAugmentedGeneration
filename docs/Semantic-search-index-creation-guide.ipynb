{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search index creation \n",
    "\n",
    "### *Guide*\n",
    "\n",
    "Anton Antonov    \n",
    "September 2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to create an LLM-computed vector database over the paragraphs of relatively large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Retrieval Augmented Generation (RAG) workflow we consider:\n",
    "\n",
    "- The document collection is ingested.\n",
    "- The documents are split into chunks of relevant sizes.\n",
    "- Large Language Model (LLM) embedding vectors are obtained for all chunks.\n",
    "- A vector database is created with these embedding vectors and stored locally. Multiple local databases can be created.\n",
    "- A relevant local database is imported for use.\n",
    "- An input query is provided to a retrieval system.\n",
    "- The retrieval system retrieves relevant documents based on the query.\n",
    "- The top K documents are selected for further processing.\n",
    "- The model is fine-tuned using the selected documents.\n",
    "- The fine-tuned model generates an answer based on the query.\n",
    "- The output answer is presented to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Mermaid-JS component diagram that shows the components of performing the Retrieval Augmented Generation (RAG) workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph LocalVDB[Local Folder]\n",
    "        A(Vector Database 1)\n",
    "        B(Vector Database 2)\n",
    "        C(Vector Database N)\n",
    "    end\n",
    "    ID[Ingest document collection]\n",
    "    SD[Split Documents]\n",
    "    EV[Get LLM Embedding Vectors]\n",
    "    CD[Create Vector Database]\n",
    "    ID --> SD --> EV --> CD\n",
    "\n",
    "    CD -.- CArray[[CArray<br>representation]]\n",
    "\n",
    "    CD -.-> |export| LocalVDB\n",
    "\n",
    "    subgraph Creation\n",
    "        ID\n",
    "        SD\n",
    "        EV\n",
    "        CD\n",
    "    end\n",
    "\n",
    "    LocalVDB -.- JSON[[JSON<br>representation]]\n",
    "\n",
    "    LocalVDB -.-> |import|D[Ingest Vector Database]\n",
    " \n",
    "    D -.- CArray\n",
    "    F -.- |nearest neighbors<br>distance function|CArray\n",
    "    D --> E\n",
    "    E[/User Query/] --> F[Retrieval]\n",
    "    F --> G[Document Selection]\n",
    "    G -->|Top K documents| H(Model Fine-tuning)\n",
    "    H --> I[[Generation]]\n",
    "    I <-.-> LLM{{LLM}}\n",
    "    I -->J[/Output Answer/]\n",
    "    G -->|Top K passages| K(Model Fine-tuning)\n",
    "    K --> I\n",
    "\n",
    "    subgraph RAG[Retrieval Augmented Generation]\n",
    "        D \n",
    "        E\n",
    "        F\n",
    "        G\n",
    "        H\n",
    "        I\n",
    "        J\n",
    "        K\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this diagram:\n",
    "\n",
    "- Document collections are ingested, processed, and corresponding vector databases are made.\n",
    "  - LLM embedding models are used for obtain the vectors.\n",
    "- There are multiple local vector databases that are stored and maintained locally.\n",
    "- A vector database from the local collection is selected and ingested.\n",
    "- An input query provided by the user initiates the RAG workflow.\n",
    "- The workflow then proceeds with: \n",
    "  - retrieval\n",
    "  - document selection\n",
    "  - model fine-tuning\n",
    "  - answer generation\n",
    "  - presenting the final output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use Data::Importers;\n",
    "use LLM::Functions;\n",
    "use XDG::BaseDirectory :terms;\n",
    "\n",
    "use LLM::RetrievalAugmentedGeneration;\n",
    "use LLM::RetrievalAugmentedGeneration::VectorDatabase;\n",
    "\n",
    "use Data::Reshapers;\n",
    "use Data::Summarizers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Ingest text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the transcript of the (3.5 hours) discussion [CWv2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chars => 245233 words => 36863 lines => 7107)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my $url = 'https://podscripts.co/podcasts/modern-wisdom/747-eric-weinstein-why-does-the-modern-world-make-no-sense';\n",
    "my $url = 'https://podscripts.co/podcasts/modern-wisdom/833-eric-weinstein-are-we-on-the-brink-of-a-revolution';\n",
    "my $txtEN = data-import($url, 'plaintext');\n",
    "\n",
    "text-stats($txtEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the \"proper transcript\" part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chars => 242067 words => 36490 lines => 7048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $txtEN2 = $txtEN.substr($txtEN.index('Starting point is 00:00:00'));\n",
    "text-stats($txtEN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into paragraphs and make the paragraphs compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my @paragraphs = $txtEN2.split(/ 'Starting point is' \\h+ [\\d ** 2]+ % ':' /):g;\n",
    "@paragraphs .= map({ $_.subst(/\\n+/, \"\\n\"):g});\n",
    "@paragraphs.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li> \n",
       "                                         that we need to do.\n",
       "Another version of this, by the way, is some giant\n",
       "percentage of the population says, I don&#39;t\n",
       "understand your argument when they say, when they\n",
       "really mean I don&#39;t accept your argument.\n",
       "For example, you could ask me, I don&#39;t, you could say, Eric, I don&#39;t accept your argument. For example, you could ask me,\n",
       "you could say, Eric, I don&#39;t understand antisemitism. Jews do so much, they contribute to society.\n",
       "I would say, I understand antisemitism.\n",
       "                                            </li><li> \n",
       "                                         Well, there are a lot of bad conspiracy theorists.\n",
       "There are a lot of losers and a lot of morons\n",
       "and a lot of idiots who imagine that lizard people\n",
       "are controlling everything.\n",
       "And so you try to make it look like the people who,\n",
       "well, let me give you an example.\n",
       "The moon landing and the JFK assassination\n",
       "are not in the same category.\n",
       "                                            </li><li> \n",
       "                                         of them, they&#39;re brilliant. Their hearts are in the right place. But if we get into an\n",
       "argument about Ovo testes, they&#39;re going to lose as two biologists and I&#39;m going to win.\n",
       "It&#39;s not all about motility of gametes. Uh, the world will keep throwing curve balls at\n",
       "you and you have to begin from a heart open place to say, some of us are shit out of\n",
       "luck because we fall in edge categories.\n",
       "And so I stand by everything that you and I did last time.\n",
       "It&#39;s a difficult place to be.\n",
       "And if you have to simplify it as to boys or boys, women or women, you&#39;re not getting\n",
       "                                            </li><li> \n",
       "                                         It doesn&#39;t matter how many documents you look at, you&#39;ll still never believe that that was true.\n",
       "It&#39;s so insane.\n",
       "This retconning, this mass lighting,\n",
       "gas lighting at global scale,\n",
       "it is mind blowing to me that this is done on the internet\n",
       "when everything is held together.\n",
       "Why?\n",
       "Because the entire internet is obsessed\n",
       "                                            </li></ul>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% html\n",
    "@paragraphs.pick(4) ==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Make vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an empty vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase(:id(\"045f467c-193f-4df6-bec3-790d6c83ca64\"), :name(\"No833\"), :elements(0), :sources(0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $vdbObj = LLM::RetrievalAugmentedGeneration::VectorDatabase.new(name => 'No833');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an LLM access specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my $conf = llm-configuration(\"ChatGPT\", model => 'text-embedding-002');\n",
    "my $conf = llm-configuration(\"Gemini\");\n",
    "\n",
    "$conf.Hash.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the semantic index for the vector database object (an profile it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to make the semantic search index: 202.806121507 seconds.\n"
     ]
    }
   ],
   "source": [
    "my $tstart = now;\n",
    "$vdbObj.create-semantic-search-index(@paragraphs, method => 'by-max-tokens', max-tokens => 2048, e => $conf):embed;\n",
    "my $tend = now;\n",
    "say \"Time to make the semantic search index: {$tend - $tstart} seconds.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the vector database object is exported in a sub-directory of [`$XDG_DATA_HOME`](https://specifications.freedesktop.org/basedir-spec/latest/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sub-directory\n",
    "my $dirname = data-home.Str ~ '/raku/LLM/SemanticSearchIndex';\n",
    "\n",
    "# The exported vector database base file name\n",
    "my $basename = \"SemSe-{$vdbObj.id}.json\";\n",
    "\n",
    "# Corresponding IO:Path object\n",
    "my $file = IO::Path.new(:$dirname, :$basename);\n",
    "\n",
    "# Check for existence\n",
    "$file.f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export path is saved in the vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$file.Str eq $vdbObj.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li><table border=\"1\"><tr><th>038.0</th><td>try and see how you feel as well. Best of all, there is a no questions asked refund policy with an unlimited duration so you can buy it for as long as you well. Best of all, there is a no questions asked refund policy with an unlimited duration. So you can buy it for as long as you want, try it all. And if you do not like it for any reason, they&#39;ll give you your money back and you don&#39;t even need to return the box. That&#39;s how confident they are that you love it. Right now you can get a free sample pack</td></tr></table></li><li><table border=\"1\"><tr><th>263.0</th><td>So how can it be the case that therapy, all therapy is bad because it allows you or causes you to focus on your yourself and your issues. But you also include in that CBT, something which is unbelievably practical and shows up as an evidence-based intervention for lots of people&#39;s disorders. Is it that, and then it&#39;s the, is it that you step in to soften the blow? So throughout that episode in particular, I had to ask these questions. And then as I watch the guests, I get to this point, which is exactly the reason</td></tr></table></li><li><table border=\"1\"><tr><th>353.0</th><td>When I did the Terence Howard thing at Joe&#39;s request, um, it generated a lot of interest and a lot of heat. I got a ton of criticism. Why would you sit down with a pseudo scientist? You&#39;re normalizing this behavior. Terence Howard is actually playing with all sorts of geometric shapes and dualities between geometric shapes that even professional mathematicians couldn&#39;t figure out. Neil Grass-Thyssen says, I don&#39;t know where these come from. Um, I didn&#39;t know where the conversation would head.</td></tr></table></li><li><table border=\"1\"><tr><th>405.0</th><td>of John Maynard Keynes, subsequent development influence to a large degree by a name I can&#39;t pronounce. I think that there was a lot of whose family comes from the far left, you recognize certain sorts of commonalities. I&#39;m sure she would see them in me. Um, the democratic party is not communist. I don&#39;t think that that&#39;s right. That&#39;s the critique of many of my right-wing friends, but it is welcomed in a lot of neo-Marxian thought.</td></tr></table></li></ul>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% html\n",
    "$vdbObj.items.pairs.pick(4).sort(*.key) ==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dimensions and data type of the obtained vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions : (441 768)\n",
      "data type  : Assoc(Vector(Atom((Str)), 441), Tuple([(Any) => 441], 441), 441)\n"
     ]
    }
   ],
   "source": [
    "say \"dimensions : \", $vdbObj.vectors.&dimensions;\n",
    "say \"data type  : \", deduce-type($vdbObj.vectors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov, \n",
    "[\"Outlier detection in a list of numbers\"](https://rakuforprediction.wordpress.com/2022/05/29/outlier-detection-in-a-list-of-numbers/),\n",
    "(2022),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[LLM::Prompts Raku package](https://github.com/antononcube/Raku-LLM-Prompts),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[ML::FindTextualAnswer Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[Math::Nearest Raku package](https://github.com/antononcube/Raku-Math-Nearest),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[Math::DistanceFunctions Raku package](https://github.com/antononcube/Raku-Math-DistanceFunctions),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[Statistics::OutlierIdentifiers Raku package](https://github.com/antononcube/Raku-Statistics-OutlierIdentifiers),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "\n",
    "[CWv1] Chris Williamson,\n",
    "[\"Eric Weinstein - Why Does The Modern World Make No Sense? (4K)\"](https://www.youtube.com/watch?v=p_swB_KS8Hw),\n",
    "(2024),\n",
    "[YouTube/@ChrisWillx](https://www.youtube.com/@ChrisWillx).   \n",
    "([transcript](https://podscripts.co/podcasts/modern-wisdom/747-eric-weinstein-why-does-the-modern-world-make-no-sense).)\n",
    "\n",
    "[CWv2] Chris Williamson,\n",
    "[\"Eric Weinstein - Are We On The Brink Of A Revolution? (4K)\"](https://www.youtube.com/watch?v=PYRYXhU4kxM),\n",
    "(2024),\n",
    "[YouTube/@ChrisWillx](https://www.youtube.com/@ChrisWillx).   \n",
    "([transcript](https://podscripts.co/podcasts/modern-wisdom/833-eric-weinstein-are-we-on-the-brink-of-a-revolution).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
